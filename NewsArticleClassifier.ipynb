{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load news articles\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles = []\n",
    "categories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('./bbc-fulltext/business'):\n",
    "    with open('./bbc-fulltext/business/'+file,'rb') as f:\n",
    "        news_articles.append(f.read())\n",
    "        categories.append('business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('./bbc-fulltext/entertainment/'):\n",
    "    with open('./bbc-fulltext/entertainment/'+file,'rb') as f:\n",
    "        news_articles.append(f.read())\n",
    "        categories.append('entertainment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('./bbc-fulltext/politics'):\n",
    "    with open('./bbc-fulltext/politics/'+file,'rb') as f:\n",
    "        news_articles.append(f.read())\n",
    "        categories.append('politics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('./bbc-fulltext/sport/'):\n",
    "    with open('./bbc-fulltext/sport/'+file,'rb') as f:\n",
    "        news_articles.append(f.read())\n",
    "        categories.append('sport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('./bbc-fulltext/tech/'):\n",
    "    with open('./bbc-fulltext/tech/'+file,'rb') as f:\n",
    "        news_articles.append(f.read())\n",
    "        categories.append('tech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_news_list = []\n",
    "for news in news_articles:\n",
    "    news = news.decode('utf-8','ignore')\n",
    "    news = re.sub(r\"\\d\",\" \",news)\n",
    "    news = re.sub(r\"\\W\",\" \",news)\n",
    "    news = re.sub(r\"\\.\",\" \",news)\n",
    "    news = re.sub(r\"\\s+[a-z0-9]\\s+\",\" \",news)\n",
    "    news = re.sub(r\"\\s+\",\" \",news)\n",
    "    news = news.lower()\n",
    "    preprocessed_news_list.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cactus diet deal for phytopharm a slimming aid made from southern african cactus is set to be developed by uk firm phytopharm and unilever anglo dutch food giant unilever will help the pharmaceutical firm develop the snacks containing hoodia extract phytopharm shares jumped on the news with analysts saying sales of m year were possible the plant licensed to phytopharm in has been used for thousands of years by the sans bushmen of the kalahari desert to stave off hunger studies have reportedly shown the plant curbs appetite instead of reducing calorific intake like many existing products phytopharm will receive an initial fee of from unilever out of potential total of as well as future royalties on product sales under the deal production of the hoodia cactus at phytopharm nursery in south africa will also rise from eight million plants to potentially hundreds of millions said phytopharm chief executive richard dixey the firm had initially hoped to market slimming drug from hoodia with pfizer but the research collaboration came to an end in analysts said unilever could launch the new products in this deal goes long way to restoring the market faith in phytopharm pipeline after the pfizer exit said analyst erling refsum at nomura '"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_news_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "12.0\n",
      "13.0\n",
      "14.0\n",
      "15.0\n",
      "16.0\n",
      "17.0\n",
      "18.0\n",
      "19.0\n",
      "20.0\n",
      "21.0\n",
      "22.0\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords\n",
    "filtered_news_list = []\n",
    "i=0\n",
    "for news in preprocessed_news_list:\n",
    "    sentence = []\n",
    "    words = nltk.word_tokenize(news)\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            sentence.append(word)\n",
    "    if i%100 == 0: \n",
    "        print(i/100)\n",
    "    filtered_news_list.append(sentence)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_news_list[1196])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmz_news_list = []\n",
    "for news in filtered_news_list:\n",
    "    sentence = []\n",
    "    for word in news:\n",
    "        new_word = word\n",
    "        pos = nltk.pos_tag([word])\n",
    "        if pos[0][1].lower()[0] == 'n':\n",
    "            new_word = lemmatizer.lemmatize(word,'n')\n",
    "        elif pos[0][1].lower()[0] == 'v':\n",
    "            new_word = lemmatizer.lemmatize(word,'v')\n",
    "        elif pos[0][1].lower()[0] == 'j':\n",
    "            new_word = lemmatizer.lemmatize(word,'a')\n",
    "        sentence.append(new_word)\n",
    "    lmz_news_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('enc_labels.pickle','wb') as f:\n",
    "    pickle.dump(enc_labels,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkz = Tokenizer(num_words=20000)\n",
    "tkz.fit_on_texts(lmz_news_list)\n",
    "tkz_sequences = tkz.texts_to_sequences(lmz_news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = pad_sequences(tkz_sequences,maxlen=250,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21847"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tkz.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading pretrained word2vec model\n",
    "import gensim\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word,i in word_index.items():\n",
    "    if word in w2v_model.vocab:\n",
    "        vec = w2v_model[word]\n",
    "        embedding_matrix[i] = vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding_matrix.pickle','rb') as f:\n",
    "    embedding_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the labels\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {'category' : categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.DataFrame(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_labels = pd.get_dummies(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_business</th>\n",
       "      <th>category_entertainment</th>\n",
       "      <th>category_politics</th>\n",
       "      <th>category_sport</th>\n",
       "      <th>category_tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_business  category_entertainment  category_politics  \\\n",
       "0                  1                       0                  0   \n",
       "1                  1                       0                  0   \n",
       "2                  1                       0                  0   \n",
       "3                  1                       0                  0   \n",
       "4                  1                       0                  0   \n",
       "\n",
       "   category_sport  category_tech  \n",
       "0               0              0  \n",
       "1               0              0  \n",
       "2               0              0  \n",
       "3               0              0  \n",
       "4               0              0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_labels = enc_labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2225, 250), (2225, 5))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape,enc_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding,Dense,Conv1D,MaxPooling1D,Flatten,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],input_length=250,trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv1D(256,3,activation='relu',padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling1D(pool_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv1D(128,3,activation='relu',padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling1D(pool_size=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(256,activation='relu',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(5,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 300)          6554400   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 250, 256)          230656    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 125, 128)          98432     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               1016064   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 7,900,837\n",
      "Trainable params: 1,346,437\n",
      "Non-trainable params: 6,554,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, enc_labels, test_size=0.3, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1557 samples, validate on 668 samples\n",
      "Epoch 1/5\n",
      "1557/1557 [==============================] - 4s 2ms/step - loss: 0.0462 - acc: 0.9878 - val_loss: 0.3392 - val_acc: 0.8952\n",
      "Epoch 2/5\n",
      "1557/1557 [==============================] - 4s 2ms/step - loss: 0.0493 - acc: 0.9827 - val_loss: 0.0833 - val_acc: 0.9686\n",
      "Epoch 3/5\n",
      "1557/1557 [==============================] - 4s 2ms/step - loss: 0.0106 - acc: 0.9987 - val_loss: 0.0882 - val_acc: 0.9701\n",
      "Epoch 4/5\n",
      "1557/1557 [==============================] - 4s 2ms/step - loss: 0.0054 - acc: 0.9994 - val_loss: 0.0887 - val_acc: 0.9701\n",
      "Epoch 5/5\n",
      "1557/1557 [==============================] - 4s 2ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0944 - val_acc: 0.9701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f38e592e400>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "          epochs=5, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
